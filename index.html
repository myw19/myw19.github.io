<!doctype html>
<html><head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Homepage of Mengyue Wu</title>
<link href="AboutPageAssets/styles/aboutPageStyle.css" rel="stylesheet" type="text/css">

<!--The following script tag downloads a font from the Adobe Edge Web Fonts server for use within the web page. We recommend that you do not modify it.-->
<script>var __adobewebfontsappname__="dreamweaver"</script><script src="http://use.edgefonts.net/montserrat:n4:default;source-sans-pro:n2:default.js" type="text/javascript"></script>
</head>

<body>
<!-- Header content -->
<header>
  <div class="profileLogo"> 
    <!-- Profile logo. Add a img tag in place of <span>. -->
    <img src="AboutPageAssets/images/x_lance.png" alt="sample" height="149" width="500"> </div>
  <div class="profilePhoto"> 
    <!-- Profile photo --> 
    <img src="AboutPageAssets/images/profile.png" alt="sample" height="259" width="259"> </div>
  <!-- Identity details -->
  <section class="profileHeader">
    <h1>Mengyue WU 吴梦玥</h1>
    <h3>Digital Mental Health and Auditory Analysis</h3>
    <hr>
    <p>Hi hi! This is Mengyue 梦玥 [IPA：[məŋ4 y̯ɛ4], both high falling tones], interested in everything about human speech and perception/production. Specifically, my current research mainly lies on <strong>Rich Audio Analysis</strong>, though I always reckon this is an invented term. It does not have a definition per se, however I refer it to everything in audio processing excluding speech recognition. I'd like to stick to it - my very first student (co-supervised PhD, Heinrich) at SJTU has a funny username "richman" and my very first learned French sentence is "Je suis riche" (pls blame Duolingo for boosting our ego)!<br>
	    Due to work requirements (probably only existing in my head), I'll present a very official biography below. <br>
	    However just feel free to drop me a line if you share any similar interests/curiosity towards audio/acoustics/language/psychology/psychiatry/neuroscience etc.<br> 
	    Have a chat - should be fun (not guaranteed on your side)!</p>
  </section>
</header>
<!-- content -->
<section class="mainContent"> 
  <section class="section1">
    <h2 class="sectionTitle">basic information&nbsp;</h2>
    <hr class="sectionTitleRule">
    <hr class="sectionTitleRule2">
    <div class="section1Content">
	  <p>Associate Professor, PhD Supervisor</p>
	  <a href="https://x-lance.sjtu.edu.cn/">X-LANCE LAB</a><br/><br/>
	  <a href="https://www.cs.sjtu.edu.cn/">Department of Computer Science and Engineering</a>
	  <p>Shanghai Jiao Tong University</p>
      <p><span>Address :</span> 3-543 SEIEE Building, 800 Dongchuan Road, Shanghai 200240, China</p>
      <p><span>Email :</span> mengyuewu@sjtu.edu.cn</p>
      <p>A bit more info: I got a dual degree of B.Sc and B.A. from Beijing Normal University and PhD from Phonetics Lab, The University of Melbourne and spent quite bit of time at MARCS Institute for Brain, Behaviour & Development. </p>
      <p>&nbsp;</p>
    </div>
  </section>

    <!-- Replicate the above Div block to add more title and company details --> 
  </section>	
	<section class="section2">
    <h2 class="sectionTitle">Research</h2>
    <hr class="sectionTitleRule">
    <hr class="sectionTitleRule2">
    <!-- First Title & company details  -->
	<article class="section2Content">
		<p class="sectionContent">SJTU X-LANCE Lab 上海交通大学跨媒体语言智能实验室丰富音频研究组</p>
		<p class="sectionContent"><strong>Environment Sound:</strong></p>
		<ul class="sectionContent">
		<li>Sound event and scene detection, generation (almost every student interested in genreation now..)</li>
		<li>Audio caption, bridging the gap between audio analysis and natural language description</li>
		<li>Audio-visual event detection</li>
		<li>Animal language decoding, now puppy language, in expansion</li>
		</ul>
		<p class="sectionContent"><strong>Human Speech: medical application</strong> </p>
		<ul class="sectionContent">
		<li>Speech emotion analysis</li>
		<li>Depression&#x2F;Parkinson’s&#x2F;Alzheimer’s disease detection</li>
		<li>Acoustic-based disease diagnosis, e.g. coughing, voice, heartsound…</li>
		</ul>
		<p class="sectionContent"><strong>Digital Mental Health: Text and Dialogue</strong> </p>
		<ul class="sectionContent">
		<li>LLM for mental health</li>
		<li>Social media based disease detection</li>
	<li>Dialogue based depression/anxiety diagnosis and therapy - building psychiatrist and therapist bots! Fun!</li>
		</ul>
		<p class="sectionContent"><strong>Open-Sourced Data and Models</strong> </p>
		<ul class="sectionContent">
		<li><a href="https://arxiv.org/abs/2402.18409">CogBench: A Cognitive Evaluation Benchmark of Image Reasoning and Description for LLM</a></li>
		<li><a href="https://aclanthology.org/2022.emnlp-main.677.pdf">PsySym: An Interpretable Mental Health Symptom Detection Dataset and Model</a></li>
		<li><a href="https://aclanthology.org/2023.emnlp-main.562.pdf">Detection of Multiple Mental Disorders from Social Media with Two-Stream Psychiatric Experts</a></li>
		<li><a href="https://arxiv.org/abs/2311.09189">PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health</a></li>
		<li><a href="https://arxiv.org/abs/2305.13614">PsyDial: LLM-empowered chatbots for psychiatrist and patient simulation: application and evaluation</a></li>
		<li><a href="https://aclanthology.org/2022.emnlp-main.156.pdf">D4: A Chinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat</a> - we built a system supporting spoken dialogue for depression diagnosis on this!</li>
	</article>
    <!-- Replicate the above Div block to add more title and company details --> 
  </section>
	<section class="section2">
    <h2 class="sectionTitle">Publications. Full list please see <a href="https://scholar.google.com/citations?user=9cvRM5wAAAAJ&hl=en">Google Scholar</a></h2>
    <hr class="sectionTitleRule3">
	<p class="sectionContent"><strong>Selected Journal Papers</strong></p>
	<ol class="sectionContent">
        <li>Zhi Chen, Yuncong Liu, Lu Chen, Su Zhu, <strong>Mengyue Wu</strong> and Kai Yu. OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue. Transactions of the Association for Computational Linguistics, 2022, <a href="AboutPageAssets/papers/zc825-chen-tacl2022.pdf">zc825-chen-tacl2022.pdf</a></li>
        <li>Heinrich Dinkel, Shuai Wang, Xuenan Xu, <strong>Mengyue Wu</strong> and Kai Yu. Voice activity detection in the wild: A data-driven approach using teacher-student training. IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 1542-1555, 2021, <a href="AboutPageAssets/papers/hedi7-dinkel-taslp2021-2.pdf">hedi7-dinkel-taslp2021-2.pdf</a></li>
        <li>Heinrich Dinkel, <strong>Mengyue Wu</strong> and Kai Yu. Towards Duration Robust Weakly Supervised Sound Event Detection. IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 887-900, 2021, <a href="AboutPageAssets/papers/hedi7-dinkel-taslp2021.pdf">hedi7-dinkel-taslp2021.pdf</a></li>
	</ol>
	<p class="sectionContent"><strong>Selected Conference Papers</strong></p>
	<ol class="sectionContent">
        <li>Guangwei Li, Xuenan Xu, <strong>Mengyue Wu</strong> and Kai Yu. Category-Adapted Sound Event Enhancement with Weakly Labeled Data. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Singapore, Singapore, 2022, 851-855, <a href="AboutPageAssets/papers/category-adapted_sound_event_enhancement_with_weakly_labeled_data.pdf">category-adapted_sound_event_enhancement_with_weakly_labeled_data.pdf</a></li>
        <li>Guangwei Li, Xuenan Xu, <strong>Mengyue Wu</strong> and Kai Yu. Navigating Audio-Visual Event Detection Across Mismatched Modalities. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Singapore, Singapore, 2022, 1975-1979, <a href="AboutPageAssets/papers/navigating_audio-visual_event_detection_across_mismatched_modalities.pdf">navigating_audio-visual_event_detection_across_mismatched_modalities.pdf</a></li>
        <li>Siyu Lou, Xuenan Xu, <strong>Mengyue Wu</strong> and Kai Yu. Audio-Text Retrieval in Context. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Singapore, Singapore, 2022, 4793-4797, <a href="AboutPageAssets/papers/syl92-lou-icassp22.pdf">syl92-lou-icassp22.pdf</a></li>
        <li>Xuenan Xu, <strong>Mengyue Wu</strong> and Kai Yu. Diversity-controllable and Accurate Audio Captioning Based on Neural Condition. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Singapore, Singapore, 2022, 971-975, <a href="AboutPageAssets/papers/xnx98-xu-icassp22.pdf">xnx98-xu-icassp22.pdf</a></li>
        <li>Wen Wu, <strong>Mengyue Wu</strong> and Kai Yu. Climate and Weather: Inspecting Depression Detection via Emotion Recognition. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Singapore, Singapore, 2022, 6262-6266, <a href="AboutPageAssets/papers/myw19-wu-icassp22-1.pdf">myw19-wu-icassp22-1.pdf</a></li>
        <li>Zelin Zhou, Zhiling Zhang, Xuenan Xu, Zeyu Xie, <strong>Mengyue Wu</strong> and Kenny Q. Zhu. Can Audio Captions Be Evaluated with Image Caption Metrics? IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Singapore, Singapore, 2022, 981-985, <a href="AboutPageAssets/papers/myw19-wu-icassp22-2.pdf">myw19-wu-icassp22-2.pdf</a></li>
        <li>Zhiling Zhang, Siyuan Chen, <strong>Mengyue Wu</strong>, Kenny Zhu. Symptom Identification for Interpretable Detection of Multiple Mental Disorders on Social Media. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022), <a href="AboutPageAssets/papers/syc20-chen-emnlp22.pdf">syc20-chen-emnlp22.pdf</a></li>
        <li>Zhiling Zhang, Siyuan Chen, <strong>Mengyue Wu</strong>, Kenny Zhu. Psychiatric Scale Guided Risky Post Screening for Early Detection of Depression. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence (IJCAI-22), <a href="AboutPageAssets/papers/syc20-chen-ijcai22.pdf">syc20-chen-ijcai22.pdf</a></li>
        <li>Binwei Yao, Chao Shi, Likai Zou, Lingfeng Dai, <strong>Mengyue Wu</strong>, Lu Chen, Zhen Wang, and Kai Yu. 2022. D4: a Chinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2438–2459, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. (EMNLP 2022), <a href="AboutPageAssets/papers/ybw00-yao-emnlp22.pdf">ybw00-yao-emnlp22.pdf</a></li>
        <li>Pingyue Zhang, <strong>Mengyue Wu</strong>, Heinrich Dinkel and Kai Yu. DEPA: Self-Supervised Audio Embedding for Depression Detection. In Proceedings of the 29th ACM International Conference on Multimedia (ACM-MM), Virtual Event, China, 2021, 135-143, <a href="AboutPageAssets/papers/myw19-wu-mm2021.pdf">myw19-wu-mm2021.pdf</a></li>
        <li>Zhiling Zhang, Zelin Zhou, Haifeng Tang, Guangwei Li, <strong>Mengyue Wu</strong> and Kenny Q. Zhu. Enriching Ontology with Temporal Commonsense for Low-Resource Audio Tagging. In Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management (CIKM), Queensland, Australia, 2021, 3652-3656, <a href="AboutPageAssets/papers/myw19-wu-cikm2021.pdf">myw19-wu-cikm2021.pdf</a></li>
        <li>Xuenan Xu, Heinrich Dinkel, <strong>Mengyue Wu</strong> and Kai Yu. A Lightweight Framework for Online Voice Activity Detection in the Wild. Proc. Interspeech 2021, 371-375, doi: 10.21437&#x2F;Interspeech.2021-1977, <a href="AboutPageAssets/papers/xnx98-xu-is2021.pdf">xnx98-xu-is2021.pdf</a></li>
        <li>Zhi Chen, Lu Chen, Hanqi Li, Ruisheng Cao, Da Ma, <strong>Mengyue Wu</strong> and Kai Yu. Decoupled Dialogue Modeling and Semantic Parsing for Multi-Turn Text-to-SQL. Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3063–3074, August 1–6, 2021, <a href="AboutPageAssets/papers/2021.findings-acl.270.pdf">2021.findings-acl.270.pdf</a></li>
        <li>Xuenan Xu, Heinrich Dinkel, <strong>Mengyue Wu</strong>, Zeyu Xie and Kai Yu. Investigating Local and Global Information for Automated Audio Captioning with Transfer Learning. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Toronto, Ontario, Canada, 2021, 905-909, <a href="AboutPageAssets/papers/xnx98-xu-icassp21-1.pdf">xnx98-xu-icassp21-1.pdf</a></li>
        <li>Xuenan Xu, Heinrich Dinkel, <strong>Mengyue Wu</strong> and Kai Yu. Text-to-Audio Grounding: Building Correspondence Between Captions and Sound Events. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Toronto, Ontario, Canada, 2021, 606-610, <a href="AboutPageAssets/papers/xnx98-xu-icassp21-2.pdf">xnx98-xu-icassp21-2.pdf</a></li>
        <li>Xuenan Xu, Heinrich Dinkel, <strong>Mengyue Wu</strong> and Kai Yu. Audio Caption in a Car Setting with a Sentence-Level Loss. In The 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), Hong Kong, China, 2021, 1-5, <a href="AboutPageAssets/papers/xnx98-xu-iscslp2021.pdf">xnx98-xu-iscslp2021.pdf</a></li>
        <li>Yefei Chen, Heinrich Dinkel, <strong>Mengyue Wu</strong> and Kai Yu. Voice activity detection in the wild via weakly supervised sound event detection. In 21st Annual Conference of the International Speech Communication Association (InterSpeech), Shanghai, China, 2020, 3665-3669, <a href="AboutPageAssets/papers/hedi7-dinkel-is2020.pdf">hedi7-dinkel-is2020.pdf</a></li>
        <li>Xuenan Xu, Heinrich Dinkel, <strong>Mengyue Wu</strong> and Kai Yu. A CRNN-GRU Based Reinforcement Learning Approach to Audio Captioning. In The 5th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE), Tokyo, Japan, 2020, 225-229, <a href="AboutPageAssets/papers/xnx98-xu-dcase2020.pdf">xnx98-xu-dcase2020.pdf</a></li>
        <li>Rui Qian, Di Hu, Heinrich Dinkel, <strong>Mengyue Wu</strong>, Ning Xu and Weiyao Lin. Multiple Sound Sources Localization from Coarse to Fine. The European Conference on Computer Vision (ECCV), Glasgow, 2020, <a href="AboutPageAssets/papers/hedi7-dinkel-eccv2020.pdf">hedi7-dinkel-eccv2020.pdf</a></li>
        <li>Rui Qian, Di Hu, Heinrich Dinkel, <strong>Mengyue Wu</strong>, Ning Xu and Weiyao Lin. A Two-Stage Framework for Multiple Sound-Source Localization. CVPR Sight and Sound Workshop, 2020, <a href="AboutPageAssets/papers/hedi7_dinkel_cvprw2020.pdf">hedi7_dinkel_cvprw2020.pdf</a></li>
        <li><strong>Mengyue Wu</strong>, Heinrich Dinkel and Kai Yu. Audio Caption: Listen and Tell. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK, 2019, 830-834, <a href="AboutPageAssets/papers/myw19-wu-icassp2019.pdf">myw19-wu-icassp2019.pdf</a></li>
	</ol>
  </section>
<section class="section2">
    <h2 class="sectionTitle">Teaching</h2>
    <hr class="sectionTitleRule">
    <hr class="sectionTitleRule2">
    <!-- First Title & company details  -->
    <article class="section2Content">
	  <p class="sectionContent"> AI3611 Deep Learning: A Practical Course on Perception and Cognition</p>
	  <p class="sectionContent"> CS3309 Computer Ethics</p>
	  <p class="sectionContent"> GE6001 Scientific Writing, Integrity and Ethics</p>
    </article>
	
<footer>
  <hr>
  <p class="footerDisclaimer">2022  Copyrights - <span>All Rights Reserved</span></p>
  <p class="footerNote">Mengyue Wu - <span>Email me</span></p>
</footer>
</body>
</html>
